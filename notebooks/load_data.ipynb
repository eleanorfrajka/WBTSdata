{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from WBTS and save in output_dir\n",
    "\n",
    "Set the `output_dir` in `config.yaml` before starting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This assumes you have already installed the package using\n",
    "``` \n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "An alternative could be:\n",
    "```\n",
    "import pathlib\n",
    "import sys\n",
    "script_dir = pathlib.Path().parent.absolute()\n",
    "parent_dir = script_dir.parents[0]\n",
    "sys.path.append(str(parent_dir))\n",
    "sys.path.append(str(parent_dir) + 'load_data')\n",
    "```\n",
    "\n",
    "which assumes that you're running the notebook from within `WBTSdata/notebooks/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the functions to load the calibration files and the \n",
    "from load_data import load_cal_files, load_vel_files, merge_datasets, tools\n",
    "from load_data import missing_datetime_2005_05 as mdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print basepath and safepath that are defined in the configuration file. The basepath should contain the data of the WBTS and the safepath is the directory here the created files will be stored in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tools.get_config()\n",
    "input_dir = config['input_dir']\n",
    "output_dir = config['output_dir']\n",
    "print('Input directory: ',input_dir,'\\nOutput directory: ',output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all directories of the calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list_ADCP = merge_datasets.dir_list_ADCP(input_dir)\n",
    "dir_list_CTD = merge_datasets.dir_list_CTD(input_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make directories within the safepath for the ADCP, CTD and the merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(output_dir, 'CTD'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'ADCP'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'Merged'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create all dataset for each year and safe it in the dedicated file in safepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the data for each individual year\n",
    "# Set the directory for yaml files as the root directory + 'load_data/' --> Could be in 'config/' instead\n",
    "if 0:\n",
    "    for i in dir_list_CTD:\n",
    "        ds = load_cal_files.create_Dataset(i, config)\n",
    "        file_name = 'WBTS_' + i.split('GC_')[1][:7] + '_CTD.nc'\n",
    "        if os.path.exists(os.path.join(output_dir, 'CTD', file_name)):\n",
    "            os.remove(os.path.join(output_dir, 'CTD', file_name))\n",
    "        ds.to_netcdf(os.path.join(output_dir, 'CTD', file_name))\n",
    "        print('Saved: ', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    for i in dir_list_ADCP:\n",
    "        ds = load_vel_files.create_Dataset(i, config)\n",
    "        file_name = 'WBTS_' + i.split('GC_')[1][:7] + '_ADCP.nc'\n",
    "        if os.path.exists(os.path.join(output_dir, 'ADCP', file_name)):\n",
    "            os.remove(os.path.join(output_dir, 'ADCP', file_name))\n",
    "        ds.to_netcdf(os.path.join(output_dir, 'ADCP', file_name))\n",
    "        print('Saved: ', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge and save the datasets of ADCP and CTD for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load CTD and ADCP datasets for years having both data and merge them\n",
    "for cal_dir in dir_list_CTD:\n",
    "    year = cal_dir.split('GC_')[1][:7]\n",
    "    for vel_dir in dir_list_ADCP:\n",
    "        if year in vel_dir:\n",
    "            print('Merging CTD and ADCP data for year: ', year)\n",
    "            merged_ds = merge_datasets.merge_datasets(cal_dir, vel_dir)\n",
    "            file_name = 'WBTS_' + year + '_CTD_LADCP.nc'\n",
    "            if os.path.exists(os.path.join(output_dir, 'Merged', file_name)):\n",
    "                os.remove(os.path.join(output_dir, 'Merged', file_name))\n",
    "                print(f\"Deleted existing file: {file_name}\")\n",
    "            merged_ds.to_netcdf(os.path.join(output_dir, 'Merged', file_name))\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
